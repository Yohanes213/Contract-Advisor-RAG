import openai
from openai import OpenAI
from dotenv import load_dotenv
from pathlib import Path
import os
from query_utils import querying, embed_text
from langsmith import traceable
from langsmith.wrappers import wrap_openai
import asyncio
from logger import logger

load_dotenv()
#openapi_key = os.getenv('OPENAI_API_KEYS')

LANGCHAIN_TRACING_V2="true"
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY=os.getenv('LANGCHAIN_API_KEY')
LANGCHAIN_PROJECT="Contract Question and Answer"

#openai.api_key = os.getenv('OPENAI_API_KEYS')
openai_client = wrap_openai(OpenAI(api_key=os.getenv('OPENAI_API_KEYS')))

@traceable
def generate_response(query, model='gpt-3.5-turbo'):
    """
    Generate a response based on a user query using OpenAI's GPT-3.5 model.

    Args:
    - query (str): The user query.
    - model (str): Name of the OpenAI model to use (default: 'gpt-3.5-turbo').

    Returns:
    - list: List of response messages generated by the model.
    """
    try:
        messages = [
            {
                "role": "system",
                "content": """You are a helpful assistant expert in law contract data.
                            You should answer the query based on the content you were given."""
            },
            {"role": "user", "content": query}
        ]

        response = openai_client.chat.completions.create(
            model=model,
            messages=messages
        )

        content = response.choices[0].message.content
        content = content.split("\n")
        return content
    except Exception as e:
        logger.error(f"Error generating response for query '{query}': {str(e)}")
        return []

async def main():
    """
    Main function to embed a query, query documents, generate a response, and print results.
    """
    try:
        query = "Can the subject pay the Advisor?"
        embedded_text = await embed_text(query)

        result = await querying(embedded_text)

        retrieved_docs = [match['metadata']['text'] for match in result['matches']]

        prompt = f"User question: {query}\n\n"
        for i, doc in enumerate(retrieved_docs, 1):
            prompt += f"Document {i}: {doc}\n\n"

        prompt += "Based on the above documents, answer the user's question."
        print(prompt)

        response = generate_response(prompt)
        print(response)
    except Exception as e:
        logger.error(f"Error in main function: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())
