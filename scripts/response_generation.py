from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
#import openai
from dotenv import load_dotenv
from langsmith import traceable
from langsmith.wrappers import wrap_openai
from langchain import hub
from langchain_openai import ChatOpenAI
import os
import asyncio
from langchain.embeddings.openai import OpenAIEmbeddings
from pinecone import Pinecone as PineconeClient
from langchain_pinecone import PineconeVectorStore
from query_expansion import expand_query_multiple, expand_query_hypothetical
from langchain_core.prompts.prompt import PromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import HumanMessagePromptTemplate



load_dotenv()

LANGCHAIN_TRACING_V2 = "true"
LANGCHAIN_ENDPOINT = "https://api.smith.langchain.com"
LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")
LANGCHAIN_PROJECT = "Contract Question and Answer"

openai_key = os.getenv("OPENAI_API_KEYS")
pinecone_key = os.getenv("PINECONE_API_KEYS")

chatopenai_client = ChatOpenAI(api_key=os.getenv("OPENAI_API_KEYS"), temperature=0)



def retrive():
    """
    Initialize Pinecone client and setup vector store retriever.

    Returns:
        Pinecone retriever object configured for contract question and answer retrieval.
    """
    pc = PineconeClient(pinecone_key)
    index_name = "lawquestionandanswer"
    embed_model = OpenAIEmbeddings(
        model="text-embedding-ada-002", openai_api_key=openai_key
    )

    vectordb = PineconeVectorStore(
        embedding=embed_model,
        pinecone_api_key=pinecone_key,
        index_name=index_name,
        namespace="Robinson",
    )

    retriver = vectordb.as_retriever(
        search_type="mmr", search_kwargs={"k": 2, "lambda_mult": 0.25}
    )

    return retriver


async def generate_response(query):
    """
    Generate a response to a given query using LangChain and OpenAI.

    Args:
        query (str): The query string about the contract.

    Returns:
        str: Response generated by the system.
    """

    retriver = retrive()

    # Combine system and user messages into a single list
    prompt_template = PromptTemplate(
        input_variables=['context', 'query'],
        template="""
            You are an AI expert specialized in contract analysis and advisory services.
            Your goal is to assist users by providing precise and helpful answers based on a contract document(context). 
            The document in question is an Advisory Services Agreement. 
            Given the user query, generate a well-structured and informative response by extracting relevant information from the document. 
            Ensure the response is concise, accurate, and directly addresses the query.
            Don't make the response long. Make the response short and precise. 
            If you know got the answer from a specific section, specify it like Under this section and write the section number.
            question: {query} context: {context}
        """
    )
    #            Don't start you response by 'Based on the Advisory Aggreement document provided'. Just write the answer only.

    
    prompt = ChatPromptTemplate(
        input_variables=['context', 'query'],
        messages=[
            HumanMessagePromptTemplate(prompt=prompt_template)
        ]
    )   

    chain = (
        {"context": retriver, "query": RunnablePassthrough()}
        | prompt
        | chatopenai_client
        | StrOutputParser()
    )
    # from langchain.chains import LLMChain
    # chain = LLMChain(llm = chatopenai_client, prompt = prompt)
    response = chain.invoke(query)
    return response

if __name__ == "__main__":
    #query = "Who are the parties to the Agreement and what are the their defined names?"
    query1 = "Who are the parties to the Agreement and what are their defined names?"

    # hypthotetical_asnwer = expand_query_hypothetical(query1)



    # joint_query = f"{query1} {hypthotetical_asnwer}"
    # print(joint_query)
    response = asyncio.run(generate_response(query1))

    print(response)